# -*- coding: utf-8 -*-
"""Multi-Layer Perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/jpete17/e3d0a42fcc6f65aa93f1388a9df9fba7/ann_mlp.ipynb
"""

'''
Created on 23-Apr-2020

@author: joel peter
'''

from google.colab import files
uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import io
import numpy as np
import datetime

import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set()

import keras
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional
from keras.models import Sequential
from keras.utils.np_utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from keras import regularizers


from tensorflow.python.keras import Sequential
from tensorflow.python.keras import layers
from tensorflow.python.keras import regularizers
from tensorflow.python.keras.preprocessing.text import Tokenizer
from keras import optimizers

Fea_Eng_df = pd.read_csv(io.BytesIO(uploaded['user1_cleaned_v2.0.csv']), encoding = "ISO-8859-1", parse_dates = ['date'])

Fea_Eng_df.info()

#impute null values
Fea_Eng_df.name.fillna('Unknown', inplace = True)
Fea_Eng_df.glucose.fillna('0.0', inplace = True)

#Convert strings to lower-case 

Fea_Eng_df['refined_activity'] = Fea_Eng_df.activity2.astype(str).str.lower()

Fea_Eng_df.describe()

Fea_Eng_df[Fea_Eng_df.dtypes[(Fea_Eng_df.dtypes=="float64")|(Fea_Eng_df.dtypes=="int64")]
                        .index.values].hist(figsize=[11,11])

'''Standardisation of values'''

scaled_features = Fea_Eng_df.copy()
col_names = ['steps', 'calories', 'glucose', 'distance', 'heart_rate_imputed']
features = scaled_features[col_names]
scaler = StandardScaler().fit(features.values)
features = scaler.transform(features.values)

Fea_Eng_df.head()

#Remove these two for activity distribution plot
Fea_Eng_df = Fea_Eng_df[Fea_Eng_df.activity2 != 'Sleeping']
Fea_Eng_df = Fea_Eng_df[Fea_Eng_df.activity2 != 'Unrecognised']

Fea_Eng_df['activity2'].value_counts().plot(kind='barh',
                                   title='Activity Type in hours')
plt.show()
# Better understand how the recordings are spread across the different
# users who participated in the study

''' Replace the standardised feature back to dataframe'''

scaled_features[col_names] = features
scaled_features.head()

''' Drop these dumb useless feature - activity is repetitve'''
scaled_features.drop(columns=['minute_ID', 'activity'], inplace = True)

''' Change the dataset from string/object to categorical'''

scaled_features['name'] = scaled_features.name.astype(str).str.lower()
scaled_features['name'] = pd.Categorical(scaled_features['name'])

''' One hot encoding - as i said, just one line Borah'''

df_onehot = pd.get_dummies(scaled_features['name'], prefix = 'category')

''' Do this if there is NAN in last row'''

scaled_features = scaled_features.iloc[0:24888]

''' append the one hot encodede values back to df'''

scaled_features = pd.concat([scaled_features, df_onehot], axis=1)

''' Remove columns to be fed into Y'''

scaled_features_oh = scaled_features.drop(columns=['activity2', 'date', 'name', 'refined_activity'])
scaled_features_oh.info()

'one hot encoding the activity to feed as target variable'
#target_onehot_df = scaled_features[['refined_activity']] 
#target_onehot_df = pd.get_dummies(target_onehot_df['refined_activity'])
target_onehot_df.info()

'''
X - Independent varibale
Y - Target varibale(Activities)
'''

X = scaled_features_oh.values #Features
Y = target_onehot_df.values #Label

#Y_raw = scaled_features[['refined_activity']].values #Label
#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)

print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))

model = Sequential() # The Sequential model is a linear stack of layers.


model.add(layers.Dense(50 , activation='relu')) 
#model.add(layers.Dropout(0.5)) 
model.add(layers.Dense(32,activation='relu'))
#model.add(layers.Dropout(0.5))
model.add(layers.Dense(15, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=100, validation_split=0.2)

test_results = model.evaluate(X_test, y_test, verbose=1)

from keras.applications import VGG16
#loading the saved model
#we are using the complete architecture thus include_top=True
model = VGG16(weights='imagenet',include_top=True)
#show the summary of model
model.summary()

from sklearn.metrics import f1_score, recall_score, precision_score

from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, confusion_matrix

classification_report(y_test, predictions)

yhat_classes = model.predict_classes(X_test, verbose=0)

''' Without regularization'''

test_results

loss_train = history.history['loss']
loss_val = history.history['val_loss']
epochs = range(0,115)
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
epochs = range(0,115)
plt.plot(epochs, accuracy, 'g', label='Training Accuracy')
plt.plot(epochs, val_accuracy, 'b', label='validation accuracy ')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

print(type(cross_val_score))

loss = history.history['loss']

epochs = range(1,len(loss)+1)

plt.plot(epochs,loss,'bo',label='Training loss')
plt.title('Training loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.figure()
acc = history.history['accuracy']
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.title('Training accuracy')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend()
plt.show()

test_results = model.evaluate(X_test, y_test, verbose=1)
print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')

''' For cross validation'''

from keras import models
from keras import layers
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification

def create_network():
    
    # Start neural network
    network = models.Sequential()

    # Add fully connected layer with a ReLU activation function
    network.add(layers.Dense(units=50, activation='relu'))

    # Add fully connected layer with a ReLU activation function
    network.add(layers.Dense(units=35, activation='relu'))

    # Add fully connected layer with a sigmoid activation function
    network.add(layers.Dense(units=15, activation='sigmoid'))

    # Compile neural network
    network.compile(loss='categorical_crossentropy', # Cross-entropy
                    optimizer='adamax', # Root Mean Square Propagation
                    metrics=['accuracy']) # Accuracy performance metric
    
    # Return compiled network
    return network

neural_network = KerasClassifier(build_fn=create_network, 
                                 epochs=50, 
                                 batch_size=100, 
                                 verbose=0)

cross_val_score(neural_network, X, Y, cv=5)